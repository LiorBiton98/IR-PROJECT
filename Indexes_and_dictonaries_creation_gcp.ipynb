{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "51cf86c5",
      "metadata": {
        "id": "51cf86c5"
      },
      "source": [
        "# **Imports & Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf199e6a",
      "metadata": {
        "id": "bf199e6a",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Setup",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f56ecd",
      "metadata": {
        "id": "d8f56ecd",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Imports",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47900073",
      "metadata": {
        "id": "47900073",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-pyspark-import",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "980e62a5",
      "metadata": {
        "id": "980e62a5",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bucket_name",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Access to the bucket\n",
        "bucket_name = '209502079'\n",
        "full_path = f\"gs://{bucket_name}/\"\n",
        "paths=[]\n",
        "\n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name)\n",
        "\n",
        "# Define prefixes to filter out specific blob names\n",
        "prefixes_to_ignore = ['graphframes.sh', 'postings_gcp', 'pagerank', 'title', 'body', 'id_to_title_dict', 'title_lengths', 'document_lengths', 'queries_train']\n",
        "\n",
        "for b in blobs:\n",
        "    # Check if the blob name does not start with any of the prefixes to ignore\n",
        "    if not any(b.name.startswith(prefix) for prefix in prefixes_to_ignore):\n",
        "        paths.append(full_path + b.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c523e7",
      "metadata": {
        "id": "e4c523e7",
        "scrolled": false,
        "outputId": "40e68610-e90e-4c4a-955b-1c4dbb13eb65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parquetFile = spark.read.parquet(*paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82881fbf",
      "metadata": {
        "id": "82881fbf",
        "outputId": "03fac37e-f3b1-42c8-d329-be1ed8fe0af4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6348910"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Count number of wiki pages\n",
        "parquetFile.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121fe102",
      "metadata": {
        "id": "121fe102",
        "outputId": "327fe81b-80f4-4b3a-8894-e74720d92e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inverted_index_gcp.py\r\n"
          ]
        }
      ],
      "source": [
        "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c101a8",
      "metadata": {
        "id": "57c101a8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# adding our python module to the cluster\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c259c402",
      "metadata": {
        "id": "c259c402"
      },
      "outputs": [],
      "source": [
        "from inverted_index_gcp import InvertedIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5540c727",
      "metadata": {
        "id": "5540c727"
      },
      "source": [
        "### **index functions**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ad8fea",
      "metadata": {
        "id": "f3ad8fea",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-token2bucket",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def word_count(text, id):\n",
        "  ''' Count the frequency of each word in `text` (tf) that is not included in\n",
        "  `all_stopwords` and return entries that will go into our posting lists.\n",
        "  Parameters:\n",
        "  -----------\n",
        "    text: str\n",
        "      Text of one document\n",
        "    id: int\n",
        "      Document id\n",
        "  Returns:\n",
        "  --------\n",
        "    List of tuples\n",
        "      A list of (token, (doc_id, tf)) pairs\n",
        "      for example: [(\"Anarchism\", (12, 5)), ...]\n",
        "  '''\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  # Count the frequency of each token in the text\n",
        "  token_counts = Counter(tokens)\n",
        "  # Filter out stopwords\n",
        "  token_counts = {token: count for token, count in token_counts.items() if token not in all_stopwords}\n",
        "  # Convert counts to (token, (doc_id, tf)) format\n",
        "  results = [(token, (id, tf)) for token, tf in token_counts.items()]\n",
        "  return results\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "  ''' Returns a sorted posting list by wiki_id.\n",
        "  Parameters:\n",
        "  -----------\n",
        "    unsorted_pl: list of tuples\n",
        "      A list of (wiki_id, tf) tuples\n",
        "  Returns:\n",
        "  --------\n",
        "    list of tuples\n",
        "      A sorted posting list.\n",
        "  '''\n",
        "  # Sort the posting list by wiki_id\n",
        "  sorted_pl = sorted(unsorted_pl, key=lambda x: x[0])\n",
        "  return sorted_pl\n",
        "\n",
        "def calculate_df(postings):\n",
        "  ''' Takes a posting list RDD and calculate the df for each token.\n",
        "  Parameters:\n",
        "  -----------\n",
        "    postings: RDD\n",
        "      An RDD where each element is a (token, posting_list) pair.\n",
        "  Returns:\n",
        "  --------\n",
        "    RDD\n",
        "      An RDD where each element is a (token, df) pair.\n",
        "  '''\n",
        "  # Count the number of documents in which each token appears\n",
        "  df_rdd = postings.map(lambda x: (x[0], len(x[1])))\n",
        "  return df_rdd\n",
        "\n",
        "def partition_postings_and_write(postings, base_dir, bucket_name='209502079'):\n",
        "  ''' A function that partitions the posting lists into buckets, writes out\n",
        "  all posting lists in a bucket to disk, and returns the posting locations for\n",
        "  each bucket. Partitioning should be done through the use of `token2bucket`\n",
        "  above. Writing to disk should use the function  `write_a_posting_list`, a\n",
        "  static method implemented in inverted_index_colab.py under the InvertedIndex\n",
        "  class.\n",
        "  Parameters:\n",
        "  -----------\n",
        "    postings: RDD\n",
        "      An RDD where each item is a (w, posting_list) pair.\n",
        "  Returns:\n",
        "  --------\n",
        "    RDD\n",
        "      An RDD where each item is a posting locations dictionary for a bucket. The\n",
        "      posting locations maintain a list for each word of file locations and\n",
        "      offsets its posting list was written to. See `write_a_posting_list` for\n",
        "      more details.\n",
        "  '''\n",
        "  def write_bucket(bucket_id, token_postings):\n",
        "      \"\"\"Write all token postings for a bucket to disk and return their locations.\"\"\"\n",
        "      posting_locs = InvertedIndex.write_a_posting_list((bucket_id, token_postings), base_dir, bucket_name)\n",
        "      return posting_locs\n",
        "\n",
        "  # Partition postings into buckets based on token2bucket_id function\n",
        "  bucketed_postings = postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1])))\n",
        "\n",
        "  # Write each bucket to disk and return posting locations for each bucket\n",
        "  bucket_posting_locs = bucketed_postings.groupByKey().map(lambda x: write_bucket(x[0], list(x[1])))\n",
        "\n",
        "  return bucket_posting_locs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8272e59",
      "metadata": {
        "id": "d8272e59"
      },
      "source": [
        "# **index for title**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dded770e",
      "metadata": {
        "id": "dded770e"
      },
      "outputs": [],
      "source": [
        "# Create a folder named \"title_index\" if it doesn't exist\n",
        "base_dir_title = 'title_index'\n",
        "if not os.path.exists(base_dir_title):\n",
        "    os.makedirs(base_dir_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c96da10",
      "metadata": {
        "id": "5c96da10"
      },
      "outputs": [],
      "source": [
        "wiki_pages_for_work = parquetFile\n",
        "title_text_pairs = wiki_pages_for_work.select(\"title\", \"id\").rdd\n",
        "\n",
        "# Tokenize titles, count occurrences, and filter stopwords\n",
        "title_word_counts = title_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "\n",
        "# Partitioning Postings and Writing to Disk\n",
        "title_postings = title_word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "title_posting_locs_list = partition_postings_and_write(title_postings, base_dir='title_index/posting').collect()\n",
        "\n",
        "# Merging Posting Locations\n",
        "title_posting_locs = defaultdict(list)\n",
        "for title_posting_loc in title_posting_locs_list:\n",
        "    for k, v in title_posting_loc.items():\n",
        "        title_posting_locs[k].extend(v)\n",
        "\n",
        "# Create a new InvertedIndex instance for titles\n",
        "title_inverted = InvertedIndex()\n",
        "title_inverted.posting_locs = title_posting_locs\n",
        "title_inverted.df = calculate_df(title_postings).collectAsMap()\n",
        "\n",
        "# Writing Global index for title to Disk\n",
        "title_inverted.write_index(base_dir_title, 'index_for_title')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d51852a4",
      "metadata": {
        "id": "d51852a4"
      },
      "outputs": [],
      "source": [
        "index_src = \"title_index/index_for_title.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9d1fec",
      "metadata": {
        "id": "0c9d1fec"
      },
      "outputs": [],
      "source": [
        "!gsutil ls -lh $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe61f1ae",
      "metadata": {
        "id": "fe61f1ae"
      },
      "source": [
        "# **index for body**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb833936",
      "metadata": {
        "id": "eb833936"
      },
      "outputs": [],
      "source": [
        "# Create a folder named \"body_index\" if it doesn't exist\n",
        "base_dir_body = 'body_index'\n",
        "if not os.path.exists(base_dir_body):\n",
        "    os.makedirs(base_dir_body)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e78c9c0",
      "metadata": {
        "id": "2e78c9c0"
      },
      "outputs": [],
      "source": [
        "wiki_pages_for_work = parquetFile\n",
        "doc_text_pairs = wiki_pages_for_work.select(\"text\", \"id\").rdd\n",
        "\n",
        "# Tokenization and Word Counting\n",
        "word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "\n",
        "# Partitioning Postings and Writing to Disk\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
        "posting_locs_list = partition_postings_and_write(postings_filtered, base_dir='body_index/posting').collect()\n",
        "\n",
        "# Merging Posting Locations\n",
        "super_posting_locs = defaultdict(list)\n",
        "for posting_loc in posting_locs_list:\n",
        "    for k, v in posting_loc.items():\n",
        "        super_posting_locs[k].extend(v)\n",
        "\n",
        "# Creating an Inverted Index\n",
        "inverted = InvertedIndex()\n",
        "inverted.posting_locs = super_posting_locs\n",
        "inverted.df = calculate_df(postings_filtered).collectAsMap()\n",
        "\n",
        "# Writing Global index for body to Disk\n",
        "inverted.write_index(base_dir_body, 'index_for_body')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f72aa1",
      "metadata": {
        "id": "a8f72aa1"
      },
      "outputs": [],
      "source": [
        "index_src = \"body_index/index_for_body.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02fa99e3",
      "metadata": {
        "id": "02fa99e3"
      },
      "outputs": [],
      "source": [
        "!gsutil ls -lh $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8d9a4ee",
      "metadata": {
        "id": "c8d9a4ee"
      },
      "source": [
        "# **PageRank**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d949fc3",
      "metadata": {
        "id": "0d949fc3"
      },
      "outputs": [],
      "source": [
        "def generate_graph(pages):\n",
        "  ''' Compute the directed graph generated by wiki links.\n",
        "  Parameters:\n",
        "  -----------\n",
        "    pages: RDD\n",
        "      An RDD where each row consists of one wikipedia articles with 'id' and\n",
        "      'anchor_text'.\n",
        "  Returns:\n",
        "  --------\n",
        "    edges: RDD\n",
        "      An RDD where each row represents an edge in the directed graph created by\n",
        "      the wikipedia links. The first entry should the source page id and the\n",
        "      second entry is the destination page id. No duplicates should be present.\n",
        "    vertices: RDD\n",
        "      An RDD where each row represents a vetrix (node) in the directed graph\n",
        "      created by the wikipedia links. No duplicates should be present.\n",
        "  '''\n",
        "  # Extract edges from anchor texts\n",
        "  edges = pages.flatMap(lambda row: [(row.id, anchor.id) for anchor in row.anchor_text])\n",
        "\n",
        "  # Remove duplicate edges\n",
        "  edges = edges.distinct()\n",
        "\n",
        "  # Extract vertices\n",
        "  vertices_src = pages.map(lambda row: Row(id=row.id))\n",
        "  vertices_dst = edges.map(lambda pair: Row(id=pair[1]))\n",
        "  vertices = vertices_src.union(vertices_dst)\n",
        "\n",
        "  # Remove duplicate vertices\n",
        "  vertices = vertices.distinct()\n",
        "\n",
        "  return edges, vertices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "803ce516",
      "metadata": {
        "id": "803ce516"
      },
      "outputs": [],
      "source": [
        "pages_links = parquetFile.select (\"id\",\"anchor_text\").rdd\n",
        "# construct the graph\n",
        "edges, vertices = generate_graph(pages_links)\n",
        "# compute PageRank\n",
        "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
        "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
        "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
        "pr = pr.sort(col('pagerank').desc())\n",
        "pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n",
        "pr.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e10c4a1a",
      "metadata": {
        "id": "e10c4a1a"
      },
      "outputs": [],
      "source": [
        "# Convert PageRank DataFrame to dictionary\n",
        "pr_dict = pr.rdd.map(lambda row: (row['id'], row['pagerank'])).collectAsMap()\n",
        "\n",
        "# Save dictionary as a pickle file\n",
        "pickle_path = 'pagerank_dict.pkl'\n",
        "with open(pickle_path, 'wb') as f:\n",
        "    pickle.dump(pr_dict, f)\n",
        "\n",
        "# Upload pickle file to GCP bucket\n",
        "!gsutil cp {pickle_path} gs://{bucket_name}/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c280274d",
      "metadata": {
        "id": "c280274d"
      },
      "source": [
        "# **dictonaries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f31dc5f3",
      "metadata": {
        "id": "f31dc5f3"
      },
      "outputs": [],
      "source": [
        "wiki_pages_for_work = parquetFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e8c3ea1",
      "metadata": {
        "id": "2e8c3ea1"
      },
      "outputs": [],
      "source": [
        "doc_text_pairs = wiki_pages_for_work.select(\"text\", \"id\").rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee5d348b",
      "metadata": {
        "id": "ee5d348b"
      },
      "outputs": [],
      "source": [
        "title_text_pairs = wiki_pages_for_work.select(\"title\", \"id\").rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ded1639f",
      "metadata": {
        "id": "ded1639f"
      },
      "outputs": [],
      "source": [
        "def calculate_document_lengths(rdd):\n",
        "    \"\"\"Calculate and return document lengths for each document in the RDD.\"\"\"\n",
        "    document_lengths = rdd.map(lambda row: (row[\"id\"], len(row[\"text\"].split()))).collectAsMap()\n",
        "    return document_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f698959",
      "metadata": {
        "id": "9f698959"
      },
      "outputs": [],
      "source": [
        "# Call the function to calculate document lengths\n",
        "document_lengths = calculate_document_lengths(doc_text_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84c64a02",
      "metadata": {
        "id": "84c64a02"
      },
      "outputs": [],
      "source": [
        "# Save dictionary as a pickle file\n",
        "pickle_path = 'document_lengths.pkl'\n",
        "with open(pickle_path, 'wb') as f:\n",
        "    pickle.dump(document_lengths, f)\n",
        "\n",
        "# Upload pickle file to GCP bucket\n",
        "!gsutil cp {pickle_path} gs://{bucket_name}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98b63107",
      "metadata": {
        "id": "98b63107"
      },
      "outputs": [],
      "source": [
        "def calculate_title_lengths(rdd):\n",
        "    \"\"\"Calculate and return title lengths for each document in the RDD.\"\"\"\n",
        "    title_lengths = rdd.map(lambda row: (row[\"id\"], len(row[\"title\"].split()))).collectAsMap()\n",
        "    return title_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "873f15e7",
      "metadata": {
        "id": "873f15e7"
      },
      "outputs": [],
      "source": [
        "# Call the function to calculate title lengths\n",
        "title_lengths = calculate_title_lengths(title_text_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee764f5",
      "metadata": {
        "id": "9ee764f5"
      },
      "outputs": [],
      "source": [
        "# Save dictionary as a pickle file\n",
        "pickle_path = 'title_lengths.pkl'\n",
        "with open(pickle_path, 'wb') as f:\n",
        "    pickle.dump(title_lengths, f)\n",
        "\n",
        "# Upload pickle file to GCP bucket\n",
        "!gsutil cp {pickle_path} gs://{bucket_name}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75285287",
      "metadata": {
        "id": "75285287"
      },
      "outputs": [],
      "source": [
        "def calculate_id_to_title_dict(rdd):\n",
        "    \"\"\"Calculate and return a dictionary mapping document IDs to titles.\"\"\"\n",
        "    id_to_title_dict = rdd.map(lambda row: (row.id, row.title)).collectAsMap()\n",
        "    return id_to_title_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3d81649",
      "metadata": {
        "id": "b3d81649"
      },
      "outputs": [],
      "source": [
        "# Call the function to calculate the dictionary\n",
        "id_to_title_dict = calculate_id_to_title_dict(title_text_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177f8d37",
      "metadata": {
        "id": "177f8d37"
      },
      "outputs": [],
      "source": [
        "# Save dictionary as a pickle file\n",
        "pickle_path = 'id_to_title_dict.pkl'\n",
        "with open(pickle_path, 'wb') as f:\n",
        "    pickle.dump(id_to_title_dict, f)\n",
        "\n",
        "# Upload pickle file to GCP bucket\n",
        "!gsutil cp {pickle_path} gs://{bucket_name}/"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}